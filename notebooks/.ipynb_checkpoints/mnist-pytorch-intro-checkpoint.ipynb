{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Every once in a while, a python library is developed that has the potential of changing the landscape in the field of Deep Learning. PyTorch is one such library. In the last few weeks, I have been dabbling a bit in PyTorch. I have been blown away by how easy it is to grasp. Among the various deep learning libraries I have used till date – PyTorch has been the most flexible and effortless of them all.\n",
    "\n",
    "Now we're going to build a larger network that can solve a (formerly) difficult problem, identifying text in an image. Here we'll use the MNIST dataset which consists of greyscale handwritten digits. Each image is 28x28 pixels, you can see a sample below\n",
    "\n",
    "Our goal is to build a neural network that can take one of these images and predict the digit in the image. Let's get straight into it\n",
    "\n",
    "[Original Notebook](https://www.kaggle.com/code/abhinand05/mnist-introduction-to-computervision-with-pytorch)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\radiu\\anaconda3\\envs\\courses\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "import matplotlib.pyplot as plt # for plotting beautiful graphs\n",
    "\n",
    "# train test split from sklearn\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import Torch \n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torchvision import transforms, models\n",
    "# from torch.utils.data import SubsetRandomSampler\n",
    "from torch.autograd import Variable\n",
    "from torch import nn, optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# plt.style.use('ggplot')\n",
    "%config InlineBackend.figure_format = 'svg'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1\n",
       "1    0\n",
       "2    1\n",
       "3    4\n",
       "4    0\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train = pd.read_csv(\"../mnist/digit-recognizer/train.csv\")\n",
    "final_test = pd.read_csv(\"../mnist/digit-recognizer/test.csv\")\n",
    "sample_sub = pd.read_csv(\"../mnist/digit-recognizer/sample_submission.csv\")\n",
    "train.label.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 42000 entries, 0 to 41999\n",
      "Columns: 785 entries, label to pixel783\n",
      "dtypes: int64(785)\n",
      "memory usage: 251.5 MB\n"
     ]
    }
   ],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Dataset\n",
    "* What we are doing here is taking the raw dataset and splitting into targets and features. Dividing by 255 makes each pixel value to scale between 0 and 1 instead of 0 and 255, which helps in training our model. This step in Machine Learning is generally known as Normalization. Then we split into train and test sets using sklearn's train_test_split function.\n",
    "\n",
    "* Converting the numpy arrays into PyTorch Tensors using from_numpy function. Don’t let the word “tensor” scare you. It is nothing more than a simple mathematical concept. Tensors are mathematical objects that generalize scalars, vectors and matrices to higher dimensions.\n",
    "\n",
    "* Batch size is set. The batch size is usually set between 64 and 256. The batch size does have an effect on the final test accuracy. One way to think about it is that smaller batches means that the number of parameter updates per epoch is greater. \n",
    "\n",
    "* To pass our data into our PyTorch models we need to convert it to a PyTorch Dataset. A Tensor Dataset in this case. \n",
    "\n",
    "* We have the training data loaded into trainloader and we can make an iterator with iter(trainloader) that can help us grab data. Later, we'll use this to loop through the dataset for training. Each time we can pull out data of the size of the batch that is defined."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate the features and labels\n",
    "targets_np = train[\"label\"]\n",
    "pureimg_train = train.drop(labels = [\"label\"], axis = 1) # drop the label column\n",
    "del train # no longer needed\n",
    "features_np = pureimg_train/255\n",
    "\n",
    "# Split into training and test set\n",
    "feature_train, feature_test, target_train, target_test = train_test_split(features_np, targets_np, test_size=0.2, random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create feature and targets tensor for train set. As you remember we need variable to accumulate gradients. Therefore, first we create a tensor, then we will create a variable.\n",
    "featuresTrain = torch.from_numpy(feature_train.values.reshape((-1,1,28,28)))\n",
    "targetsTrain = torch.from_numpy(target_train.values)\n",
    "\n",
    "# Create feature and targets tensor for test set.\n",
    "featuresTest = torch.from_numpy(feature_test.values.reshape((-1,1,28,28)))\n",
    "targetsTest = torch.from_numpy(target_test.values) # data type is long"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set batch size\n",
    "batch_size = 256\n",
    "test_batch_size = 256\n",
    "\n",
    "# Pytorch train and test sets\n",
    "train = torch.utils.data.TensorDataset(featuresTrain.float(), targetsTrain)\n",
    "test = torch.utils.data.TensorDataset(featuresTest.float(), targetsTest)\n",
    "\n",
    "# Define train and test data loaders\n",
    "train_loader = torch.utils.data.DataLoader(train, batch_size=batch_size, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test, batch_size=test_batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Visualize one of the images in data set\n",
    "# def visualize_image(data, index, pred=False, val=0):\n",
    "#     \"\"\"\n",
    "#     This function can be used to visualize the images\n",
    "#     \"\"\"\n",
    "#     plt.imshow(data[index])\n",
    "#     plt.axis(\"off\")\n",
    "#     plt.title(\"Handwritten Digit Image\")\n",
    "#     plt.show()\n",
    "#\n",
    "# visualize_image(features_np, index=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([33600, 1, 28, 28])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "featuresTrain.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Define Network Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(32, 64, 3, 1)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        self.dropout2 = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(9216, 128)\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.conv2(x)\n",
    "        x = F.relu(x)\n",
    "        x = F.max_pool2d(x, 2)\n",
    "        x = self.dropout1(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc2(x)\n",
    "        output = F.log_softmax(x, dim=1)\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and Validation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we should create our own network and train it. First we'll want to define the criterion (something like nn.CrossEntropyLoss or nn.NLLLoss) and the optimizer (typically optim.SGD or optim.Adam).\n",
    "* Make a forward pass through the network\n",
    "* Use the network output to calculate the loss\n",
    "* Perform a backward pass through the network with loss.backward() to calculate the gradients\n",
    "* Take a step with the optimizer to update the weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate our model\n",
    "model = CNNModel()\n",
    "# Define our loss function\n",
    "criterion = nn.NLLLoss()\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0015)\n",
    "\n",
    "epochs = 2\n",
    "steps = 0\n",
    "print_every = 100\n",
    "train_losses, test_losses = [], []\n",
    "\n",
    "for e in range(epochs):\n",
    "    running_loss = 0\n",
    "    for images, labels in train_loader:\n",
    "        steps += 1\n",
    "        # Prevent accumulation of gradients\n",
    "        optimizer.zero_grad()\n",
    "        # Make predictions\n",
    "        log_ps = model(images)\n",
    "        loss = criterion(log_ps, labels)\n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if steps % print_every == 0:\n",
    "            test_loss = 0\n",
    "            accuracy = 0\n",
    "\n",
    "            # Turn off gradients for validation\n",
    "            with torch.no_grad():\n",
    "                model.eval()\n",
    "                for images, labels in test_loader:\n",
    "                    log_ps = model(images)\n",
    "                    test_loss += criterion(log_ps, labels)\n",
    "\n",
    "                    ps = torch.exp(log_ps)\n",
    "                    # Get our top predictions\n",
    "                    top_p, top_class = ps.topk(1, dim=1)\n",
    "                    equals = top_class == labels.view(*top_class.shape)\n",
    "                    accuracy += torch.mean(equals.type(torch.FloatTensor))\n",
    "\n",
    "            model.train()\n",
    "\n",
    "            train_losses.append(running_loss/len(train_loader))\n",
    "            test_losses.append(test_loss/len(test_loader))\n",
    "\n",
    "            print(\"Epoch: {}/{}.. \".format(e+1, epochs),\n",
    "                  \"Training Loss: {:.3f}.. \".format(train_losses[-1]),\n",
    "                  \"Test Loss: {:.3f}.. \".format(test_losses[-1]),\n",
    "                  \"Test Accuracy: {:.3f}\".format(accuracy/len(test_loader)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oh what just happended there? I'll explain not to worry.\n",
    "\n",
    "#### Training: \n",
    "* I'm looping over the train loader, pulling out the images and labels.\n",
    "* Note that I have a line of code optimizer.zero_grad(). When you do multiple backwards passes with the same parameters, the gradients are accumulated. This means that you need to zero the gradients on each training pass or you'll retain gradients from previous training batches.\n",
    "* I have named the next variable log_ps because our model gives us back logs of class probabilities, you can take exponent to convert it to normal probabilities which I've done down below for validation. \n",
    "* We calculate the loss. Then backpropagate through the network. We then make one optimizer step. Which brings us closer and closer to the global optimum.\n",
    "\n",
    "#### Validation\n",
    "* We turn off the gradients for validation as it is not needed and saves a lot of memory and computation. Note that we should turn it back on after each step of validation.\n",
    "* We loop over the test_loader and essentially repeat some steps we have done above. Since it's validation we don't need to backpropagate. \n",
    "* The next step - With the probabilities, we can get the most likely class using the ps.topk method. This returns the $k$ highest values. Since we just want the most likely class, we can use ps.topk(1). This returns a tuple of the top-$k$ values and the top-$k$ indices. If the highest value is the fifth element, we'll get back 4 as the index.\n",
    "* Then we check if the predicted value is equal to the actual value. \n",
    "* We then calculate the percentage of correct predictions, which indeed is using the mean of our top predictions. But you cannot just use torch.mean because topk returns a byte tensor but we need a float tensor to perform torch.mean we do that in the next step.\n",
    "\n",
    "The same process is repeated over and over again. The results are printed on each step. With this simple model we're able to get about 98% accuracy on validation which is awesome, isn't it?\n",
    "\n",
    "Hope that made sense. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(test_losses, label='Validation loss')\n",
    "plt.legend(frameon=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This graph looks decent to me. We're doing fairly well for our first model in PyTorch."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference\n",
    "You can see what out model is predicting here on the test data. You can try playing around with this function for different images. \n",
    "\n",
    "As expected our model seems to do well, infact really well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def view_classify(img, ps):\n",
    "#     \"\"\"\n",
    "#     Function for viewing an image, and it's predicted classes.\n",
    "#     \"\"\"\n",
    "#     ps = ps.data.numpy().squeeze()\n",
    "#\n",
    "#     fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "#     ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
    "#     ax1.axis('off')\n",
    "#     ax2.barh(np.arange(10), ps)\n",
    "#     ax2.set_aspect(0.1)\n",
    "#     ax2.set_yticks(np.arange(10))\n",
    "#     ax2.set_yticklabels(np.arange(10))\n",
    "#     ax2.set_title('Class Probability')\n",
    "#     ax2.set_xlim(0, 1.1)\n",
    "#\n",
    "#     plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "#\n",
    "# def make_prediction(data):\n",
    "#     images, labels = next(iter(data))\n",
    "#\n",
    "#     img = images[42].view(1, 784)\n",
    "#     # Turn off gradients to speed up this part\n",
    "#     with torch.no_grad():\n",
    "#         logps = model(img)\n",
    "#\n",
    "#     # Output of the network are log-probabilities, need to take exponential for probabilities\n",
    "#     ps = torch.exp(logps)\n",
    "#     view_classify(img.view(1, 28, 28), ps)\n",
    "#\n",
    "# make_prediction(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preparing Test Data for Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_test_np = final_test.values/255\n",
    "test_tn = torch.from_numpy(final_test_np.reshape((-1,1,28,28)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create fake labels for convenience of passing into DataLoader\n",
    "## CAUTION: There are other ways of doing this, I just did it this way\n",
    "fake_labels = np.zeros(final_test_np.shape)\n",
    "fake_labels = torch.from_numpy(fake_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_tn_data = torch.utils.data.TensorDataset(test_tn.float(), fake_labels)\n",
    "\n",
    "submission_loader = torch.utils.data.DataLoader(submission_tn_data, batch_size=1, shuffle = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seeing what our model does on test data\n",
    "# make_prediction(submission_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making Predictions on Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prediction_func(data_loader):\n",
    "    # Making it submission ready\n",
    "    my_submission = [['ImageId', 'Label']]\n",
    "\n",
    "    # Turn off gradients for validation\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        image_id = 1\n",
    "        for images, _ in data_loader:\n",
    "            log_ps = model(images)\n",
    "            ps = torch.exp(log_ps)\n",
    "            top_p, top_class = ps.topk(1, dim=1)\n",
    "\n",
    "            for pred in top_class:\n",
    "                my_submission.append([image_id, pred.item()])\n",
    "                image_id += 1\n",
    "    return my_submission\n",
    "\n",
    "submission = prediction_func(submission_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df = pd.DataFrame(submission)\n",
    "submission_df.columns = submission_df.iloc[0]\n",
    "submission_df = submission_df.drop(0, axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "def penultimate_layer_extract():\n",
    "    # Extract the prediction weights and bias in the penultimate layer of CNNModel\n",
    "    penultimate_weights_pred = model.fc1.weight.data\n",
    "    penultimate_bias_pred = model.fc1.bias.data\n",
    "\n",
    "    # Convert weights and bias to numpy\n",
    "    weights_np_pred = penultimate_weights_pred.cpu().numpy()\n",
    "    bias_np_pred = penultimate_bias_pred.cpu().numpy()\n",
    "\n",
    "    # Since bias is a 1D array, reshape it to have the same number of dimensions as weights\n",
    "    bias_np_pred = bias_np_pred.reshape(-1, 1)\n",
    "\n",
    "    # Stack them horizontally to create a single numpy array\n",
    "    combined_array_pred = np.hstack((weights_np_pred, bias_np_pred))\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(combined_array_pred)\n",
    "\n",
    "    # Save to CSV\n",
    "    df.to_csv(\"intro_penultimate_layer_pred.csv\", index=False)\n",
    "\n",
    "penultimate_layer_extract()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_df.to_csv(\"../results_pytorch/my_intro_submission.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **If you like this kernel or wish to fork it, please UPVOTE to show your support.**\n",
    "\n",
    "**Authored by:**\n",
    "[Abhinand](https://www.kaggle.com/abhinand05)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
