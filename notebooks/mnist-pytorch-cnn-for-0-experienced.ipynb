{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020905,
     "end_time": "2021-01-08T02:33:07.501725",
     "exception": false,
     "start_time": "2021-01-08T02:33:07.480820",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Introduction\n",
    "Hi! ðŸ˜‰\n",
    "\n",
    "There are a lot of notesbooks and tutorials for this digital recognizer competition. This notebook tries to summarize the various techniques out there and demonstrates the process that I learn to write my first neural network. Let's go!\n",
    "\n",
    "[Original Notebook](https://www.kaggle.com/code/bronyashijiayang/mnist-pytorch-cnn-for-0-experienced)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from torch.autograd import Variable\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# plt.style.use('ggplot')\n",
    "%config InlineBackend.figure_format = 'svg'"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initialize Parameters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "# define training parameters\n",
    "N_EPOCHS = 2\n",
    "BATCH_SIZE = 88\n",
    "LEARNING_RATE = 0.001\n",
    "GAMMA_PARAM = 0.7\n",
    "ALPHA_PARAM = 0.9"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "papermill": {
     "duration": 0.019579,
     "end_time": "2021-01-08T02:33:07.541376",
     "exception": false,
     "start_time": "2021-01-08T02:33:07.521797",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## MNIST Data\n",
    "Before writing the code, we need to understand the dataset. I recommend to read the data description from the contest page: [https://www.kaggle.com/c/digit-recognizer/data](http://) \n",
    "\n",
    "Some key points:\n",
    "- For each image: height 28 pixels * width 28 pixels = 784 pixels\n",
    "- Pixel value: from 0 (lightest) to 255 (darkest)\n",
    "- For the whole training dataset: 785 columns\n",
    "- The first column is label, the rest corresponds to one of the 784 pixels, so each row is a complete image\n",
    "- To convert each image's pixel coordinate (i, j) to pixel number x (from 0 to 783), do x = i * 28 + j"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.020184,
     "end_time": "2021-01-08T02:33:07.582319",
     "exception": false,
     "start_time": "2021-01-08T02:33:07.562135",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Let's take a closer look to the data. If you have never tried data preprocessing, check out this course: [https://www.kaggle.com/learn/data-cleaning](http://)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T02:33:07.629901Z",
     "iopub.status.busy": "2021-01-08T02:33:07.629298Z",
     "iopub.status.idle": "2021-01-08T02:33:14.337626Z",
     "shell.execute_reply": "2021-01-08T02:33:14.336484Z"
    },
    "papermill": {
     "duration": 6.733319,
     "end_time": "2021-01-08T02:33:14.337758",
     "exception": false,
     "start_time": "2021-01-08T02:33:07.604439",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load data\n",
    "train = pd.read_csv(\"../mnist/digit-recognizer/train.csv\")\n",
    "test = pd.read_csv(\"../mnist/digit-recognizer/test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T02:33:14.387603Z",
     "iopub.status.busy": "2021-01-08T02:33:14.387019Z",
     "iopub.status.idle": "2021-01-08T02:33:14.405719Z",
     "shell.execute_reply": "2021-01-08T02:33:14.406188Z"
    },
    "papermill": {
     "duration": 0.048001,
     "end_time": "2021-01-08T02:33:14.406319",
     "exception": false,
     "start_time": "2021-01-08T02:33:14.358318",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   label  pixel0  pixel1  pixel2  pixel3  pixel4  pixel5  pixel6  pixel7  \\\n0      1       0       0       0       0       0       0       0       0   \n1      0       0       0       0       0       0       0       0       0   \n2      1       0       0       0       0       0       0       0       0   \n3      4       0       0       0       0       0       0       0       0   \n4      0       0       0       0       0       0       0       0       0   \n\n   pixel8  ...  pixel774  pixel775  pixel776  pixel777  pixel778  pixel779  \\\n0       0  ...         0         0         0         0         0         0   \n1       0  ...         0         0         0         0         0         0   \n2       0  ...         0         0         0         0         0         0   \n3       0  ...         0         0         0         0         0         0   \n4       0  ...         0         0         0         0         0         0   \n\n   pixel780  pixel781  pixel782  pixel783  \n0         0         0         0         0  \n1         0         0         0         0  \n2         0         0         0         0  \n3         0         0         0         0  \n4         0         0         0         0  \n\n[5 rows x 785 columns]",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>label</th>\n      <th>pixel0</th>\n      <th>pixel1</th>\n      <th>pixel2</th>\n      <th>pixel3</th>\n      <th>pixel4</th>\n      <th>pixel5</th>\n      <th>pixel6</th>\n      <th>pixel7</th>\n      <th>pixel8</th>\n      <th>...</th>\n      <th>pixel774</th>\n      <th>pixel775</th>\n      <th>pixel776</th>\n      <th>pixel777</th>\n      <th>pixel778</th>\n      <th>pixel779</th>\n      <th>pixel780</th>\n      <th>pixel781</th>\n      <th>pixel782</th>\n      <th>pixel783</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>1</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n      <td>0</td>\n    </tr>\n  </tbody>\n</table>\n<p>5 rows Ã— 785 columns</p>\n</div>"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# take a look of first several rows of the training dataset\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T02:33:14.453528Z",
     "iopub.status.busy": "2021-01-08T02:33:14.451990Z",
     "iopub.status.idle": "2021-01-08T02:33:14.494478Z",
     "shell.execute_reply": "2021-01-08T02:33:14.494901Z"
    },
    "papermill": {
     "duration": 0.067753,
     "end_time": "2021-01-08T02:33:14.495023",
     "exception": false,
     "start_time": "2021-01-08T02:33:14.427270",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing data\n",
    "train.isnull().sum().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021067,
     "end_time": "2021-01-08T02:33:14.537180",
     "exception": false,
     "start_time": "2021-01-08T02:33:14.516113",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We can see most of the pixels are white (0 in value). Thus, this dataset consists white background with black numbers (I thought it is black background). There is no missing data, so we can continue."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T02:33:14.585805Z",
     "iopub.status.busy": "2021-01-08T02:33:14.585014Z",
     "iopub.status.idle": "2021-01-08T02:33:14.670155Z",
     "shell.execute_reply": "2021-01-08T02:33:14.669481Z"
    },
    "papermill": {
     "duration": 0.112085,
     "end_time": "2021-01-08T02:33:14.670301",
     "exception": false,
     "start_time": "2021-01-08T02:33:14.558216",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# separate label and pixels for the training set\n",
    "# the testing set does not contain lables\n",
    "labels = train[\"label\"]\n",
    "pureimg_train = train.drop(labels = [\"label\"], axis = 1) # drop the label column\n",
    "del train # no longer needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021126,
     "end_time": "2021-01-08T02:33:14.713483",
     "exception": false,
     "start_time": "2021-01-08T02:33:14.692357",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "Next, we need to normalize training and testing data to make pixel value ranging from \\[0, 1\\], because CNN will converge faster on \\[0, 1\\]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T02:33:14.765957Z",
     "iopub.status.busy": "2021-01-08T02:33:14.765366Z",
     "iopub.status.idle": "2021-01-08T02:33:14.971478Z",
     "shell.execute_reply": "2021-01-08T02:33:14.970929Z"
    },
    "papermill": {
     "duration": 0.231934,
     "end_time": "2021-01-08T02:33:14.971597",
     "exception": false,
     "start_time": "2021-01-08T02:33:14.739663",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# normalize train and test\n",
    "norm_train = pureimg_train/255\n",
    "norm_test = test/255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021452,
     "end_time": "2021-01-08T02:33:15.014971",
     "exception": false,
     "start_time": "2021-01-08T02:33:14.993519",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "We randomly separate part of the training set as validation set. Here I choose 10% of the data to be validation set. You can choose whatever fraction you want. The reason that we can choose randomly is that the digits are evenly spread between 0 to 9. If we have an uneven amount of each digit, we cannot randomly split.\n",
    "\n",
    "Validation set is used for hyperparameter tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T02:33:15.065173Z",
     "iopub.status.busy": "2021-01-08T02:33:15.064575Z",
     "iopub.status.idle": "2021-01-08T02:33:16.264869Z",
     "shell.execute_reply": "2021-01-08T02:33:16.263765Z"
    },
    "papermill": {
     "duration": 1.227362,
     "end_time": "2021-01-08T02:33:16.264988",
     "exception": false,
     "start_time": "2021-01-08T02:33:15.037626",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "'\\nfeature: non-label part of the image\\ntarget: what we want to get, so it is the label of the image\\nwe should not reshape just yet because this function by default separates data row wise by axis 0\\nso the shape of 2D training data set array should be one image per row\\notherwise it will be separated incorrectly\\n'"
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# split the training data into training and validation set\n",
    "feature_train, feature_validate, target_train, target_validate = train_test_split(norm_train, labels, test_size=0.1, random_state=0)\n",
    "\n",
    "'''\n",
    "feature: non-label part of the image\n",
    "target: what we want to get, so it is the label of the image\n",
    "we should not reshape just yet because this function by default separates data row wise by axis 0\n",
    "so the shape of 2D training data set array should be one image per row\n",
    "otherwise it will be separated incorrectly\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T02:33:16.317192Z",
     "iopub.status.busy": "2021-01-08T02:33:16.316628Z",
     "iopub.status.idle": "2021-01-08T02:33:17.879491Z",
     "shell.execute_reply": "2021-01-08T02:33:17.878149Z"
    },
    "papermill": {
     "duration": 1.592034,
     "end_time": "2021-01-08T02:33:17.879610",
     "exception": false,
     "start_time": "2021-01-08T02:33:16.287576",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# change data frame to numpy, and then to tensor form\n",
    "# time to reshape\n",
    "\n",
    "Test = torch.from_numpy(norm_test.values.reshape((-1,1,28,28)))\n",
    "featuresTrain = torch.from_numpy(feature_train.values.reshape((-1,1,28,28)))\n",
    "targetsTrain = torch.from_numpy(target_train.values)\n",
    "featuresValidation = torch.from_numpy(feature_validate.values.reshape((-1,1,28,28)))\n",
    "targetsValidation = torch.from_numpy(target_validate.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.021722,
     "end_time": "2021-01-08T02:33:17.923525",
     "exception": false,
     "start_time": "2021-01-08T02:33:17.901803",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Data Augmentation\n",
    "In order to have higher accuracy, we need more data. We can do data augmentation, rotation and filp of the existing images, to increase data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T02:33:18.172395Z",
     "iopub.status.busy": "2021-01-08T02:33:18.171698Z",
     "iopub.status.idle": "2021-01-08T02:33:18.174951Z",
     "shell.execute_reply": "2021-01-08T02:33:18.174468Z"
    },
    "papermill": {
     "duration": 0.034757,
     "end_time": "2021-01-08T02:33:18.175050",
     "exception": false,
     "start_time": "2021-01-08T02:33:18.140293",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define own dataset\n",
    "class MNISTDataset(Dataset):\n",
    "    \"\"\"MNIST dataset\"\"\"\n",
    "    \n",
    "    def __init__(self, feature, target=None, transform=None):\n",
    "        \n",
    "        self.X = feature\n",
    "        self.y = target\n",
    "            \n",
    "        self.transform = transform\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.X)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # training\n",
    "        if self.transform is not None:\n",
    "            return self.transform(self.X[idx]), self.y[idx]\n",
    "        # testing\n",
    "        elif self.y is None:\n",
    "            return [self.X[idx]]\n",
    "        # validation\n",
    "        return self.X[idx], self.y[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T02:33:18.227812Z",
     "iopub.status.busy": "2021-01-08T02:33:18.227249Z",
     "iopub.status.idle": "2021-01-08T02:33:18.384655Z",
     "shell.execute_reply": "2021-01-08T02:33:18.383766Z"
    },
    "papermill": {
     "duration": 0.187253,
     "end_time": "2021-01-08T02:33:18.384769",
     "exception": false,
     "start_time": "2021-01-08T02:33:18.197516",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# define transform operation\n",
    "data_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomAffine(degrees=45, translate=(0.1, 0.1), scale=(0.8, 1.2)),\n",
    "    transforms.ToTensor()])\n",
    "\n",
    "# create dataset\n",
    "# train_set = MNISTDataset(featuresTrain.float(), targetsTrain, transform=data_transform)\n",
    "# validate_set = MNISTDataset(featuresValidation.float(), targetsValidation)\n",
    "# test_set = MNISTDataset(Test.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T02:33:18.436648Z",
     "iopub.status.busy": "2021-01-08T02:33:18.435682Z",
     "iopub.status.idle": "2021-01-08T02:33:18.589343Z",
     "shell.execute_reply": "2021-01-08T02:33:18.588697Z"
    },
    "papermill": {
     "duration": 0.181533,
     "end_time": "2021-01-08T02:33:18.589470",
     "exception": false,
     "start_time": "2021-01-08T02:33:18.407937",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# if choose not to do data augmentation\n",
    "# create dataset like this, move this cell to the end of the section before data loading\n",
    "train_set = torch.utils.data.TensorDataset(featuresTrain.float(), targetsTrain)\n",
    "validate_set = torch.utils.data.TensorDataset(featuresValidation.float(), targetsValidation)\n",
    "test_set = torch.utils.data.TensorDataset(Test.float())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T02:33:18.640839Z",
     "iopub.status.busy": "2021-01-08T02:33:18.640070Z",
     "iopub.status.idle": "2021-01-08T02:33:18.642467Z",
     "shell.execute_reply": "2021-01-08T02:33:18.643022Z"
    },
    "papermill": {
     "duration": 0.030859,
     "end_time": "2021-01-08T02:33:18.643128",
     "exception": false,
     "start_time": "2021-01-08T02:33:18.612269",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# load the data\n",
    "train_loader = torch.utils.data.DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True)\n",
    "validate_loader = torch.utils.data.DataLoader(validate_set, batch_size=BATCH_SIZE, shuffle=False)\n",
    "test_loader = torch.utils.data.DataLoader(test_set, batch_size=BATCH_SIZE, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.023628,
     "end_time": "2021-01-08T02:33:18.689532",
     "exception": false,
     "start_time": "2021-01-08T02:33:18.665904",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## CNN\n",
    "We are done with data. Let's build the neural network using torch.\n",
    "\n",
    "Network structure:\n",
    "1. Convolutional(Conv2D) layer: filter=32, activation=relu\n",
    "2. Convolutional(Conv2D) layer: filter=32, activation=relu\n",
    "3. Max pooling layer: kernel size=2, stride=1\n",
    "4. Dropout: rate=0.25\n",
    "5. Convolutional(Conv2D) layer: filter=64, activation=relu\n",
    "6. Convolutional(Conv2D) layer: filter=64, activation=relu\n",
    "7. Max pooling layer: kernel size=2, stride=2\n",
    "8. Dropout: rate=0.25\n",
    "9. Flatten layer\n",
    "10. Fully connected(Dense) layer\n",
    "11. Dropout: rate=0.5\n",
    "12. Fully connected(Dense) layer\n",
    "\n",
    "ðŸ¤”There must be a ton of questions regarding this structure.\n",
    "\n",
    "**Q: What is a filter?**\n",
    "\n",
    "A: A filter performs dot product with each portion of the input image, so the portion of the most matched between input and a filter will have the highest value for the output. It means certain portion of the input contains the feature represented by the filter, so we call the output a feature map. The number of filters is the number of feature maps and the number of out channels.\n",
    "\n",
    "**Q: What is activation?**\n",
    "\n",
    "A: Activation function introduces nonlinearity to the neural network. It is a function that, if the output value meets the threshold, the neuron switches on and the value is passed through, otherwise the value is not passed through.\n",
    "\n",
    "**Q: What is an inplace relu?**\n",
    "\n",
    "A: \"Inplace\" means that it will modify the input directly, without allocating any additional output. It can sometimes slightly decrease the memory usage. For this network specifically, adding inplace makes the training accuracy increases from 92 to 99.9. Weird...\n",
    "\n",
    "**Q: What is max pooling?**\n",
    "\n",
    "A: Max pooling is a way of down sampling to reduce computational cost and make the feature map more robust. This operation calculates the maximum value for each patch, for example, 2\\*2 here, of the feature map.\n",
    "\n",
    "**Q: What is stride?**\n",
    "\n",
    "A: Stride is the number of pixels shifts over the input matrix. When the stride is 1 then we move the filters to 1 pixel at a time. When the stride is 2 then we move the filters to 2 pixels at a time and so on. \n",
    "\n",
    "**Q: What is dropout?**\n",
    "\n",
    "A: During training, some number of layer outputs are randomly ignored with certain probability, for example, 0.25 and 0.5 here. Dropout regularization is for reducing overfitting and improving the generalization of deep neural networks.\n",
    "\n",
    "**Q: What is a flattern layer?**\n",
    "\n",
    "A: This layer simply flattens the array into a 1-dimensional array for inputting it to the next layer.\n",
    "\n",
    "**Q: What is a dense layer?**\n",
    "\n",
    "A: A dense layer is a fully connected layer. The layer performs a linear operation on the layerâ€™s input vector.\n",
    "\n",
    "**Q: How to calculate the dimension that goes into the first dense layer?**\n",
    "\n",
    "A: It is the last conv layer's out channels number times the size of 1 feature map. For this network specifically,it is 64\\*3\\*3.\n",
    "\n",
    "**Q: Why do you use this structure?**\n",
    "\n",
    "A: This is the simplest structure with the highest accuracy that I can find among Kaggle notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T02:33:18.754620Z",
     "iopub.status.busy": "2021-01-08T02:33:18.753221Z",
     "iopub.status.idle": "2021-01-08T02:33:18.761734Z",
     "shell.execute_reply": "2021-01-08T02:33:18.761119Z"
    },
    "papermill": {
     "duration": 0.049167,
     "end_time": "2021-01-08T02:33:18.761823",
     "exception": false,
     "start_time": "2021-01-08T02:33:18.712656",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class CNNModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNNModel, self).__init__()\n",
    "        \n",
    "        self.cnn = nn.Sequential(nn.Conv2d(in_channels=1, out_channels=32, kernel_size=5),\n",
    "                                     nn.ReLU(inplace=True),\n",
    "                                     nn.Conv2d(in_channels=32, out_channels=32, kernel_size=5),\n",
    "                                     nn.ReLU(inplace=True),\n",
    "                                     nn.MaxPool2d(kernel_size=2),\n",
    "                                     nn.Dropout(0.25),\n",
    "                                     nn.Conv2d(in_channels=32, out_channels=64, kernel_size=3),\n",
    "                                     nn.ReLU(inplace=True),\n",
    "                                     nn.Conv2d(in_channels=64, out_channels=64, kernel_size=3),\n",
    "                                     nn.ReLU(inplace=True),\n",
    "                                     nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "                                     nn.Dropout(0.25))\n",
    "        \n",
    "        self.classifier = nn.Sequential(nn.Linear(576, 256),\n",
    "                                       nn.Dropout(0.5),\n",
    "                                       nn.Linear(256, 10))\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.cnn(x)\n",
    "        x = x.view(x.size(0), -1) # flatten layer\n",
    "        output = self.classifier(x)\n",
    "        \n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T02:33:18.825018Z",
     "iopub.status.busy": "2021-01-08T02:33:18.824370Z",
     "iopub.status.idle": "2021-01-08T02:33:23.837684Z",
     "shell.execute_reply": "2021-01-08T02:33:23.836843Z"
    },
    "papermill": {
     "duration": 5.052799,
     "end_time": "2021-01-08T02:33:23.837806",
     "exception": false,
     "start_time": "2021-01-08T02:33:18.785007",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# defining the model\n",
    "model = CNNModel()\n",
    "\n",
    "# set optimizer, loss function, and learning rate reduction\n",
    "# all these parameter comes from the first notebook in citation, the author explains the reason of choosing well\n",
    "optimizer = optim.RMSprop(model.parameters(), lr=LEARNING_RATE, alpha=ALPHA_PARAM)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "lr_reduction = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3, threshold=0.0001, threshold_mode='rel', cooldown=0, min_lr=0.00001)\n",
    "\n",
    "scheduler = StepLR(optimizer, step_size=1, gamma=GAMMA_PARAM)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model = model.cuda()\n",
    "    criterion = criterion.cuda()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Initial Penultimate Layer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "outputs": [],
   "source": [
    "# Extract the initial weights and bias in the penultimate layer of CNNModel\n",
    "penultimate_weights_initial = model.classifier[0].weight.data\n",
    "penultimate_bias_initial = model.classifier[0].bias.data\n",
    "\n",
    "# Convert weights and bias to numpy\n",
    "weights_np_initial = penultimate_weights_initial.cpu().numpy()\n",
    "bias_np_initial = penultimate_bias_initial.cpu().numpy()\n",
    "\n",
    "# Since bias is a 1D array, reshape it to have the same number of dimensions as weights\n",
    "bias_np_initial = bias_np_initial.reshape(-1, 1)\n",
    "\n",
    "# Stack them horizontally to create a single numpy array\n",
    "combined_array_initial = np.hstack((weights_np_initial, bias_np_initial))\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(combined_array_initial)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"penultimate_layer_initial.csv\", index=False)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([ 0.0245, -0.0177,  0.0058, -0.0277,  0.0185,  0.0233, -0.0072, -0.0230,\n         0.0358,  0.0395,  0.0269,  0.0380, -0.0235,  0.0338, -0.0163, -0.0281,\n         0.0134,  0.0334,  0.0352,  0.0047,  0.0214, -0.0104,  0.0168, -0.0395,\n         0.0145, -0.0264,  0.0107, -0.0231,  0.0334, -0.0257,  0.0349, -0.0320,\n         0.0132,  0.0316, -0.0198,  0.0384,  0.0117, -0.0100,  0.0004, -0.0098,\n        -0.0056,  0.0200, -0.0151, -0.0285, -0.0334, -0.0405,  0.0160,  0.0165,\n        -0.0268,  0.0138,  0.0145, -0.0020, -0.0040, -0.0136, -0.0062, -0.0186,\n         0.0354, -0.0141,  0.0205, -0.0077,  0.0014,  0.0305, -0.0229, -0.0352,\n        -0.0265, -0.0290,  0.0091,  0.0242,  0.0258,  0.0115, -0.0074, -0.0398,\n         0.0401, -0.0203,  0.0045, -0.0269, -0.0021,  0.0412, -0.0006,  0.0314,\n         0.0271,  0.0065, -0.0152, -0.0233, -0.0178, -0.0138,  0.0223,  0.0141,\n         0.0292,  0.0037,  0.0077,  0.0398, -0.0402, -0.0260, -0.0395,  0.0149,\n         0.0319, -0.0393, -0.0292, -0.0234, -0.0176,  0.0265,  0.0382, -0.0008,\n        -0.0397, -0.0196, -0.0303, -0.0018,  0.0392, -0.0332,  0.0239,  0.0111,\n        -0.0151,  0.0371, -0.0225, -0.0181, -0.0315, -0.0079,  0.0016, -0.0399,\n        -0.0357,  0.0350, -0.0012,  0.0097,  0.0093, -0.0266, -0.0315,  0.0217,\n        -0.0289, -0.0325,  0.0209, -0.0031,  0.0077, -0.0409,  0.0014, -0.0168,\n         0.0410, -0.0157,  0.0024,  0.0321, -0.0219, -0.0106, -0.0234,  0.0349,\n        -0.0110,  0.0160, -0.0171, -0.0314, -0.0115, -0.0270,  0.0157, -0.0179,\n         0.0407,  0.0201, -0.0260, -0.0276,  0.0393, -0.0273, -0.0263,  0.0092,\n         0.0095, -0.0374,  0.0026,  0.0410, -0.0289,  0.0182, -0.0013,  0.0251,\n         0.0223,  0.0416, -0.0030, -0.0390,  0.0183, -0.0033,  0.0279, -0.0238,\n         0.0023,  0.0157,  0.0003,  0.0119, -0.0052, -0.0386,  0.0405, -0.0227,\n         0.0056, -0.0198, -0.0104, -0.0041,  0.0268,  0.0189,  0.0325, -0.0125,\n        -0.0240, -0.0289,  0.0031,  0.0249, -0.0253, -0.0031,  0.0223, -0.0382,\n        -0.0416, -0.0342, -0.0130,  0.0372, -0.0231,  0.0274,  0.0200,  0.0360,\n         0.0100,  0.0162,  0.0247, -0.0332,  0.0260, -0.0390, -0.0285,  0.0267,\n        -0.0414, -0.0228,  0.0407,  0.0151, -0.0071, -0.0310, -0.0222, -0.0108,\n        -0.0374,  0.0359, -0.0108, -0.0278,  0.0214, -0.0366, -0.0248,  0.0376,\n         0.0311,  0.0403,  0.0405,  0.0033,  0.0181,  0.0297,  0.0333, -0.0141,\n        -0.0183, -0.0046, -0.0197, -0.0118, -0.0372,  0.0120,  0.0129, -0.0159,\n         0.0187, -0.0200,  0.0394,  0.0081,  0.0170, -0.0231, -0.0040, -0.0272])"
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penultimate_bias_initial"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.023895,
     "end_time": "2021-01-08T02:33:23.886665",
     "exception": false,
     "start_time": "2021-01-08T02:33:23.862770",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Training and Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T02:33:23.942450Z",
     "iopub.status.busy": "2021-01-08T02:33:23.940626Z",
     "iopub.status.idle": "2021-01-08T02:33:23.943165Z",
     "shell.execute_reply": "2021-01-08T02:33:23.943655Z"
    },
    "papermill": {
     "duration": 0.032618,
     "end_time": "2021-01-08T02:33:23.943767",
     "exception": false,
     "start_time": "2021-01-08T02:33:23.911149",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for visualization\n",
    "count = 0\n",
    "loss_list = []\n",
    "iteration_list = []\n",
    "average_training_accuracy = []\n",
    "average_validation_accuracy = []\n",
    "average_training_loss = []\n",
    "average_validation_loss = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T02:33:24.002871Z",
     "iopub.status.busy": "2021-01-08T02:33:24.001580Z",
     "iopub.status.idle": "2021-01-08T02:33:24.004261Z",
     "shell.execute_reply": "2021-01-08T02:33:24.004834Z"
    },
    "papermill": {
     "duration": 0.037112,
     "end_time": "2021-01-08T02:33:24.004948",
     "exception": false,
     "start_time": "2021-01-08T02:33:23.967836",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def train(epoch):\n",
    "    print('Epoch ', epoch) # uncomment me to show verbose\n",
    "    global count\n",
    "    model.train()\n",
    "\n",
    "    for batch_idx, (data, target) in enumerate(train_loader):\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "        loss = criterion(output, target)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (batch_idx + 1)% 100 == 0:\n",
    "            # store loss and iteration\n",
    "            loss_list.append(loss.item())\n",
    "            iteration_list.append(count)\n",
    "            count += 1\n",
    "            print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(epoch, (batch_idx + 1) * len(data), len(train_loader.dataset), 100. * (batch_idx + 1) / len(train_loader), loss.item())) # uncomment me to show verbose\n",
    "\n",
    "    # Save the model weights\n",
    "    torch.save(model.state_dict(), '../models_pytorch/mnist_pytorch_model.pth')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T02:33:24.067029Z",
     "iopub.status.busy": "2021-01-08T02:33:24.065671Z",
     "iopub.status.idle": "2021-01-08T02:33:24.068246Z",
     "shell.execute_reply": "2021-01-08T02:33:24.068943Z"
    },
    "papermill": {
     "duration": 0.039546,
     "end_time": "2021-01-08T02:33:24.069072",
     "exception": false,
     "start_time": "2021-01-08T02:33:24.029526",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def evaluate(data_loader, validate=False):\n",
    "    model.eval()\n",
    "    loss = 0\n",
    "    correct = 0\n",
    "    \n",
    "    for data, target in data_loader:\n",
    "        data, target = Variable(data), Variable(target)\n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "            target = target.cuda()\n",
    "        \n",
    "        output = model(data)\n",
    "        \n",
    "        loss += F.cross_entropy(output, target, reduction='mean').item()\n",
    "\n",
    "        pred = output.data.max(1, keepdim=True)[1]\n",
    "        correct += pred.eq(target.data.view_as(pred)).cpu().sum()\n",
    "        \n",
    "    loss /= len(data_loader.dataset)\n",
    "    \n",
    "    accuracy = 100. * correct / len(data_loader.dataset)\n",
    "    \n",
    "    if not validate:\n",
    "        lr_reduction.step(loss)\n",
    "        average_training_accuracy.append(accuracy)\n",
    "        average_training_loss.append(loss)\n",
    "        print('Average training loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)'.format(loss, correct, len(data_loader.dataset), accuracy)) # uncomment me to show verbose\n",
    "    else:\n",
    "        average_validation_accuracy.append(accuracy)\n",
    "        average_validation_loss.append(loss)\n",
    "        print('Average validation loss: {:.4f}, Accuracy: {}/{} ({:.3f}%)\\n'.format(loss, correct, len(data_loader.dataset), accuracy)) # uncomment me to show verbose\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T02:33:24.129628Z",
     "iopub.status.busy": "2021-01-08T02:33:24.128912Z",
     "iopub.status.idle": "2021-01-08T02:37:44.042888Z",
     "shell.execute_reply": "2021-01-08T02:37:44.043378Z"
    },
    "papermill": {
     "duration": 259.950231,
     "end_time": "2021-01-08T02:37:44.043535",
     "exception": false,
     "start_time": "2021-01-08T02:33:24.093304",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch  1\n",
      "Train Epoch: 1 [8800/37800 (23%)]\tLoss: 0.444693\n",
      "Train Epoch: 1 [17600/37800 (47%)]\tLoss: 0.130987\n",
      "Train Epoch: 1 [26400/37800 (70%)]\tLoss: 0.276101\n",
      "Train Epoch: 1 [35200/37800 (93%)]\tLoss: 0.124884\n",
      "Average training loss: 0.0008, Accuracy: 36997/37800 (97.876%)\n",
      "Average validation loss: 0.0009, Accuracy: 4088/4200 (97.333%)\n",
      "\n",
      "Epoch  2\n",
      "Train Epoch: 2 [8800/37800 (23%)]\tLoss: 0.039121\n",
      "Train Epoch: 2 [17600/37800 (47%)]\tLoss: 0.010814\n",
      "Train Epoch: 2 [26400/37800 (70%)]\tLoss: 0.112747\n",
      "Train Epoch: 2 [35200/37800 (93%)]\tLoss: 0.060337\n",
      "Average training loss: 0.0004, Accuracy: 37379/37800 (98.886%)\n",
      "Average validation loss: 0.0005, Accuracy: 4141/4200 (98.595%)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# without data augmentation reaches 99.3, with augmentation reaches 99.2\n",
    "\n",
    "for epoch in range(1, N_EPOCHS + 1):\n",
    "    train(epoch)\n",
    "    evaluate(train_loader)\n",
    "    evaluate(validate_loader, True)\n",
    "    scheduler.step()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Training Penultimate Layer."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "# Extract the training weights and bias in the penultimate layer of CNNModel\n",
    "penultimate_weights_eval = model.classifier[0].weight.data\n",
    "penultimate_bias_eval = model.classifier[0].bias.data\n",
    "\n",
    "# Convert weights and bias to numpy\n",
    "weights_np_eval = penultimate_weights_eval.cpu().numpy()\n",
    "bias_np_eval = penultimate_bias_eval.cpu().numpy()\n",
    "\n",
    "# Since bias is a 1D array, reshape it to have the same number of dimensions as weights\n",
    "bias_np_eval = bias_np_eval.reshape(-1, 1)\n",
    "\n",
    "# Stack them horizontally to create a single numpy array\n",
    "combined_array_eval = np.hstack((weights_np_eval, bias_np_eval))\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(combined_array_eval)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"penultimate_layer_eval.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    }
   },
   "outputs": [],
   "source": [
    "# plt.plot(iteration_list,loss_list)\n",
    "# plt.xlabel(\"Number of iteration\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.title(\"Training Loss vs Number of iteration\")\n",
    "# plt.show()\n",
    "#\n",
    "# epoch_list = [i + 1 for i + 1 in range(N_EPOCHS + 1)]\n",
    "#\n",
    "# plt.plot(epoch_list, average_training_loss)\n",
    "# plt.plot(epoch_list, average_validation_loss)\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Loss\")\n",
    "# plt.title(\"Loss vs Epoch\")\n",
    "# plt.show()\n",
    "#\n",
    "# plt.plot(epoch_list, average_training_accuracy)\n",
    "# plt.plot(epoch_list, average_validation_accuracy)\n",
    "# plt.xlabel(\"Epoch\")\n",
    "# plt.ylabel(\"Accuracy\")\n",
    "# plt.title(\"Accuracy vs Epoch\")\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.02651,
     "end_time": "2021-01-08T02:37:44.097486",
     "exception": false,
     "start_time": "2021-01-08T02:37:44.070976",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T02:37:44.159379Z",
     "iopub.status.busy": "2021-01-08T02:37:44.158535Z",
     "iopub.status.idle": "2021-01-08T02:37:45.191930Z",
     "shell.execute_reply": "2021-01-08T02:37:45.191206Z"
    },
    "papermill": {
     "duration": 1.068392,
     "end_time": "2021-01-08T02:37:45.192047",
     "exception": false,
     "start_time": "2021-01-08T02:37:44.123655",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def prediciton(data_loader):\n",
    "    model.eval()\n",
    "    test_pred = torch.LongTensor()\n",
    "    \n",
    "    for i, data in enumerate(data_loader):\n",
    "        data = Variable(data[0])\n",
    "        if torch.cuda.is_available():\n",
    "            data = data.cuda()\n",
    "            \n",
    "        output = model(data)\n",
    "        \n",
    "        pred = output.cpu().data.max(1, keepdim=True)[1]\n",
    "        test_pred = torch.cat((test_pred, pred), dim=0)\n",
    "        \n",
    "    return test_pred\n",
    "\n",
    "test_pred = prediciton(train_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "tensor([[8],\n        [5],\n        [1],\n        [2],\n        [3],\n        [8],\n        [3],\n        [2],\n        [2],\n        [7]])"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_pred[0:10]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Inference"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "def view_classify(img, ps):\n",
    "    \"\"\"\n",
    "    Function for viewing an image, and it's predicted classes.\n",
    "    \"\"\"\n",
    "    ps = ps.data.numpy().squeeze()\n",
    "\n",
    "    fig, (ax1, ax2) = plt.subplots(figsize=(6,9), ncols=2)\n",
    "    ax1.imshow(img.resize_(1, 28, 28).numpy().squeeze())\n",
    "    ax1.axis('off')\n",
    "    ax2.barh(np.arange(10), ps)\n",
    "    ax2.set_aspect(0.1)\n",
    "    ax2.set_yticks(np.arange(10))\n",
    "    ax2.set_yticklabels(np.arange(10))\n",
    "    ax2.set_title('Class Probability')\n",
    "    ax2.set_xlim(0, 1.1)\n",
    "\n",
    "    plt.tight_layout()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "%matplotlib inline\n",
    "def make_prediction(data):\n",
    "    images, labels = next(iter(data))\n",
    "\n",
    "    img = images[42].view(1, 784)\n",
    "    # Turn off gradients to speed up this part\n",
    "    with torch.no_grad():\n",
    "        logps = model(img)\n",
    "\n",
    "    # Output of the network are log-probabilities, need to take exponential for probabilities\n",
    "    ps = torch.exp(logps)\n",
    "    view_classify(img.view(1, 28, 28), ps)\n",
    "\n",
    "make_prediction(test_loader)\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Preparing Test Data for Prediction"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "model.load_state_dict(torch.load('../models_pytorch/mnist_pytorch_model.pth'))\n",
    "model.eval()  # Set the model to evaluation mode\n",
    "\n",
    "# List to hold predictions\n",
    "predictions = []\n",
    "\n",
    "# Perform predictions\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        outputs = model(images)\n",
    "        labels, predicted = torch.max(outputs, 1)  # Get the index of the max log-probability\n",
    "        predictions.append(predicted.item())\n",
    "\n",
    "# Save to CSV\n",
    "df = pd.DataFrame(predictions, columns=['Predicted_Label'])\n",
    "df.to_csv('../results_pytorch/my_predictions.csv', index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "outputs": [],
   "source": [
    "# Extract the prediction weights and bias in the penultimate layer of CNNModel\n",
    "penultimate_weights_pred = model.classifier[2].weight.data\n",
    "penultimate_bias_pred = model.classifier[2].bias.data\n",
    "\n",
    "# Convert weights and bias to numpy\n",
    "weights_np_pred = penultimate_weights_pred.cpu().numpy()\n",
    "bias_np_pred = penultimate_bias_pred.cpu().numpy()\n",
    "\n",
    "# Since bias is a 1D array, reshape it to have the same number of dimensions as weights\n",
    "bias_np_pred = bias_np_pred.reshape(-1, 1)\n",
    "\n",
    "# Stack them horizontally to create a single numpy array\n",
    "combined_array_pred = np.hstack((weights_np_pred, bias_np_pred))\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(combined_array_pred)\n",
    "\n",
    "# Save to CSV\n",
    "df.to_csv(\"penultimate_layer_pred.csv\", index=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([10, 256])"
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penultimate_weights_pred.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "outputs": [
    {
     "data": {
      "text/plain": "torch.Size([10])"
     },
     "execution_count": 76,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "penultimate_bias_pred.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.026374,
     "end_time": "2021-01-08T02:37:45.245345",
     "exception": false,
     "start_time": "2021-01-08T02:37:45.218971",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T02:37:45.309680Z",
     "iopub.status.busy": "2021-01-08T02:37:45.303296Z",
     "iopub.status.idle": "2021-01-08T02:37:45.312699Z",
     "shell.execute_reply": "2021-01-08T02:37:45.313189Z"
    },
    "papermill": {
     "duration": 0.041438,
     "end_time": "2021-01-08T02:37:45.313319",
     "exception": false,
     "start_time": "2021-01-08T02:37:45.271881",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": "   ImageId  Label\n0        1      8\n1        2      5\n2        3      1\n3        4      2\n4        5      3",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>ImageId</th>\n      <th>Label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>1</td>\n      <td>8</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>2</td>\n      <td>5</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>3</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>4</td>\n      <td>2</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>5</td>\n      <td>3</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out_df = pd.DataFrame(np.c_[np.arange(1, len(train_set)+1)[:,None], test_pred.numpy()],\n",
    "                      columns=['ImageId', 'Label'])\n",
    "\n",
    "# check prediction makes sense\n",
    "out_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2021-01-08T02:37:45.372456Z",
     "iopub.status.busy": "2021-01-08T02:37:45.371852Z",
     "iopub.status.idle": "2021-01-08T02:37:45.694948Z",
     "shell.execute_reply": "2021-01-08T02:37:45.695542Z"
    },
    "papermill": {
     "duration": 0.355195,
     "end_time": "2021-01-08T02:37:45.695715",
     "exception": false,
     "start_time": "2021-01-08T02:37:45.340520",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "out_df.to_csv('../results_pytorch/my_submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "papermill": {
     "duration": 0.046241,
     "end_time": "2021-01-08T02:37:45.852593",
     "exception": false,
     "start_time": "2021-01-08T02:37:45.806352",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "## Citation\n",
    "- Notebook CNN with PyTorch (0.995 Accuracy) [https://www.kaggle.com/juiyangchang/cnn-with-pytorch-0-995-accuracy](http://)\n",
    "- Notebook from Yassine Ghouzam, PhD, Introduction to CNN Keras - 0.997 (top 6%) [https://www.kaggle.com/yassineghouzam/introduction-to-cnn-keras-0-997-top-6](http://)\n",
    "- Notebook Pytorch Tutorial for Deep Learning Lovers [https://www.kaggle.com/kanncaa1/pytorch-tutorial-for-deep-learning-lovers](http://)\n",
    "- How Do Convolutional Layers Work in Deep Learning Neural Networks? [https://machinelearningmastery.com/convolutional-layers-for-deep-learning-neural-networks/#:~:text=Convolutional%20layers%20are%20the%20major,that%20results%20in%20an%20activation.&text=The%20result%20is%20highly%20specific,detected%20anywhere%20on%20input%20images.](http://)\n",
    "- A Gentle Introduction to Pooling Layers for Convolutional Neural Networks [https://machinelearningmastery.com/pooling-layers-for-convolutional-neural-networks/](http://)\n",
    "- A Gentle Introduction to Dropout for Regularizing Deep Neural Networks [https://machinelearningmastery.com/dropout-for-regularizing-deep-neural-networks/](http://)\n",
    "- Determining size of FC layer after Conv layer in PyTorch [https://datascience.stackexchange.com/questions/40906/determining-size-of-fc-layer-after-conv-layer-in-pytorch](http://)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  },
  "papermill": {
   "duration": 283.987887,
   "end_time": "2021-01-08T02:37:47.173794",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2021-01-08T02:33:03.185907",
   "version": "2.1.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
